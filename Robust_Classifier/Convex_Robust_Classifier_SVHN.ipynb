{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"PyTorch (master)","language":"python","name":"pytorch_master"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Convex_Robust_Classifier_SVHN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"I0y1MkSPDIBc","colab_type":"text"},"source":["# ReLU-Based Robust Classifier via the Convex Outer Adversarial Polytope - SVHN Dataset\n","\n","In this notebook, two state-of-the-art CNNs of 4 convolutional layers prior to the fully-connected part are trained on the SVHN normally and robustly using the the \"Convex Outer Adversarial Polytope\" described in [Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope](https://arxiv.org/pdf/1711.00851.pdf) by Wong and Kolter. This notebook also contains the code to convert neural networks from PyTorch to Keras and vice versa for completeness with the rest of the notebooks of this project."]},{"cell_type":"code","metadata":{"id":"TT9V_R4Qcy-t","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTzNgplUCcyi","colab_type":"code","colab":{}},"source":["!pip install tensorflow==1.15.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yFdiDUpDIBd","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import seaborn\n","import matplotlib.patches as patches\n","from scipy.spatial import HalfspaceIntersection\n","%matplotlib inline\n","seaborn.set(font_scale=2)\n","seaborn.set_style(\"white\")\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import argparse\n","import torch\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Model\n","from keras.layers import Dense, Dropout, Flatten ,Input\n","from keras.layers import Conv2D, MaxPooling2D, Reshape, Add\n","from keras.metrics import categorical_accuracy\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers import Activation\n","from keras.utils.generic_utils import get_custom_objects\n","from tensorflow.python.keras import backend as K\n","from keras import models\n","from keras import layers\n","from keras.preprocessing.image import array_to_img, img_to_array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnzKKgnLDO8g","colab_type":"code","colab":{}},"source":["!pip install convex_adversarial"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-ZbkrFa_1-z","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"P6DUOVkv_3vA","colab_type":"code","colab":{}},"source":["from scipy.io import loadmat\n","import shutil\n","from sklearn.model_selection import train_test_split\n","import keras\n","\n","x_train = loadmat('/content/drive/My Drive/Colab Notebooks/SVHN/train_32x32.mat')\n","x_acc_test = loadmat('/content/drive/My Drive/Colab Notebooks/SVHN/test_32x32.mat')\n","\n","y_train = x_train['y']\n","x_train = x_train['X']\n","y_acc_test = x_acc_test['y']\n","x_acc_test = x_acc_test['X']\n","\n","x_train = np.rollaxis(x_train, 3)\n","x_acc_test = np.rollaxis(x_acc_test, 3)\n","\n","y_train = y_train[:,0]\n","y_acc_test = y_acc_test[:,0]\n","\n","y_train[y_train==10] = 0\n","y_acc_test[y_acc_test==10] = 0\n","\n","#Split between Training and Validation datasets (x_train = train & x_test = validation)\n","x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state = 100)\n","\n","# Create labels and one-hot-encoding\n","y_labels_train = y_train\n","y_labels_test = y_test\n","y_labels_acc_test = y_acc_test\n","y_train = keras.utils.to_categorical(y_train, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)\n","y_acc_test = keras.utils.to_categorical(y_acc_test, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGccRCwQBUi1","colab_type":"code","colab":{}},"source":["#Preprocess data for CNN\n","x_train_cnn = x_train.reshape(-1, 32, 32, 3)\n","x_train_cnn = x_train_cnn.astype('float32')\n","x_train_cnn /= 255.\n","\n","x_test_cnn = x_test.reshape(-1, 32, 32, 3)\n","x_test_cnn = x_test_cnn.astype('float32')\n","x_test_cnn /= 255.\n","\n","x_acc_test_cnn = x_acc_test.reshape(-1, 32, 32, 3)\n","x_acc_test_cnn = x_acc_test_cnn.astype('float32')\n","x_acc_test_cnn /= 255.\n","\n","# Save\n","np.save('x_train_cnn', x_train_cnn)\n","np.save('x_acc_test_cnn', x_acc_test_cnn)\n","np.save('x_test_cnn', x_test_cnn)\n","\n","shutil.move(\"/content/x_train_cnn.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_train_cnn.npy\")\n","shutil.move(\"/content/x_acc_test_cnn.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy\")\n","shutil.move(\"/content/x_test_cnn.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_test_cnn.npy\")\n","\n","# Save the labels\n","np.save('y_labels_train', y_labels_train)\n","np.save('y_labels_test', y_labels_test)\n","np.save('y_labels_acc_test', y_labels_acc_test)\n","np.save('y_train', y_train)\n","np.save('y_test', y_test)\n","np.save('y_acc_test', y_acc_test)\n","\n","shutil.move(\"/content/y_labels_train.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_train.npy\")\n","shutil.move(\"/content/y_labels_test.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_test.npy\")\n","shutil.move(\"/content/y_labels_acc_test.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_acc_test.npy\")\n","shutil.move(\"/content/y_train.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_train.npy\")\n","shutil.move(\"/content/y_test.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_test.npy\")\n","shutil.move(\"/content/y_acc_test.npy\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_acc_test.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39ygCfMRsLfl","colab_type":"text"},"source":["# From PyTorch to Keras"]},{"cell_type":"code","metadata":{"id":"sPrxv2abDIB-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import keras\n","\n","def pyt_to_keras(pytorch_model, keras_model):\n","  pyt_state_dict = pytorch_model.state_dict()\n","  for idx, layer in enumerate(keras_model.layers):\n","    if type(layer).__name__.endswith('Conv2D'):\n","      # Keras 2D Convolutional layer: height * width * input channels * output channels\n","      # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n","      name = layer.name\n","      weights = np.transpose(pyt_state_dict[name + '.weight'].numpy(), (2, 3, 1, 0))\n","      bias = pyt_state_dict[name + '.bias'].numpy()\n","      keras_model.layers[idx].set_weights([weights, bias])\n","    elif type(layer).__name__.endswith('Dense'):\n","      # Keras Linear Layer: input neurons * output neurons\n","      # PyTorch Linear Layer: output neurons * input neurons\n","      name = layer.name\n","      weights = np.transpose(pyt_state_dict[name + '.weight'].numpy(), (1, 0))\n","      bias = pyt_state_dict[name + '.bias'].numpy()\n","      keras_model.layers[idx].set_weights([weights, bias])\n","  return keras_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4cCXxiLS5sz","colab_type":"code","colab":{}},"source":["!pip install torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0SOFqYYsW7e","colab_type":"text"},"source":["# Train Baseline"]},{"cell_type":"code","metadata":{"id":"pAPQU5n89Jx-","colab_type":"code","colab":{}},"source":["def CNN_model():\n","  model = nn.Sequential(\n","        nn.Conv2d(3, 16,  4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 16, 4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 32, 4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(32, 32, 4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Linear(32*2*2,100),\n","        nn.ReLU(),\n","        nn.Linear(100, 10)\n","    )\n","  return model\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n","model = CNN_model()\n","model.cuda()\n","from torchsummary import summary\n","\n","summary(model, (3, 32, 32))\n","print (model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gh67XgXLYel7","colab_type":"code","colab":{}},"source":["!pip install setproctitle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vYJGZRutv55","colab_type":"code","colab":{}},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfKXoQpG4P2p","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import setproctitle\n","import argparse\n","import problems as pblm\n","from trainer import *\n","import torch\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","prefix = \"/content/SVHN_robust/\"\n","\n","train_log = open(prefix + \"train_baseline.log\", \"w\")\n","test_log = open(prefix + \"test_baseline.log\", \"w\")\n","\n","# Restore the data\n","x_train_cnn_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_train_cnn.npy')\n","\n","x_train_cnn=[]\n","for i in range(58605):\n","  test = x_train_cnn_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_train_cnn.append(np.array(test_new))\n","x_train_cnn = np.array(x_train_cnn) \n","\n","x_test_cnn_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_test_cnn.npy')\n","x_test_cnn=[]\n","for i in range(14652):\n","  test = x_test_cnn_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_test_cnn.append(np.array(test_new))\n","x_test_cnn = np.array(x_test_cnn)\n","\n","y_train=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_train.npy')\n","y_test=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_test.npy')\n","\n","tensor_x = torch.tensor(x_train_cnn, dtype=torch.float) # transform to torch tensor\n","tensor_y = torch.tensor(y_train, dtype=torch.long)\n","dataset_train = TensorDataset(tensor_x, tensor_y) # create your datset\n","train_loader = DataLoader(dataset_train,batch_size=20, shuffle=False) # create your dataloader\n","\n","tensor_x_test = torch.tensor(x_test_cnn) # transform to torch tensor\n","tensor_y_test = torch.tensor(y_test, dtype=torch.long)\n","dataset_test = TensorDataset(tensor_x_test, tensor_y_test) # create your datset\n","test_loader = DataLoader(dataset_test, batch_size=20, shuffle=False) # create your dataloader\n","\n","train_loader, test_loader = pblm.svhn_loaders(20)\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","model = CNN_model().cuda()\n","\n","opt = optim.Adam(model.parameters(), lr=0.001)\n","\n","epsilon = 0.01\n","batch_size=20\n","epochs=20\n","\n","starting_epsilon=None\n","baseline=True\n","alpha_grad=True\n","scatter_grad=True\n","\n","verbose = 0\n","\n","for epoch in range(epochs):\n","  batch_time = AverageMeter()\n","  data_time = AverageMeter()\n","  losses = AverageMeter()\n","  errors = AverageMeter()\n","\n","  model.train()\n","\n","  end = time.time()\n","  for i, (X,y) in enumerate(train_loader):\n","      X,y = X.cuda(), y.cuda()\n","      data_time.update(time.time() - end)\n","\n","      out = model(Variable(X))\n","      ce = nn.CrossEntropyLoss()(out, Variable(y))\n","      err = (out.data.max(1)[1] != y).float().sum()  / X.size(0)\n","\n","      opt.zero_grad()\n","      ce.backward()\n","      opt.step()\n","\n","      batch_time.update(time.time()-end)\n","      end = time.time()\n","      losses.update(ce.item(), X.size(0))\n","      errors.update(err, X.size(0))\n","\n","      print(epoch, i, ce.item(), err, file=train_log)\n","      if verbose and i % verbose == 0:\n","          print('Epoch: [{0}][{1}/{2}]\\t'\n","                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                'Error {errors.val:.3f} ({errors.avg:.3f})'.format(\n","                epoch, i, len(train_loader), batch_time=batch_time,\n","                data_time=data_time, loss=losses, errors=errors))\n","      train_log.flush()\n","\n","  batch_time = AverageMeter()\n","  losses = AverageMeter()\n","  errors = AverageMeter()\n","\n","  model.eval()\n","\n","  end = time.time()\n","  for i, (X,y) in enumerate(test_loader):\n","      X,y = X.cuda(), y.cuda()\n","      out = model(Variable(X))\n","      ce = nn.CrossEntropyLoss()(out, Variable(y))\n","      err = (out.data.max(1)[1] != y).float().sum()  / X.size(0)\n","\n","      # print to logfile\n","      print(epoch, i, ce.item(), err, file=test_log)\n","\n","      # measure accuracy and record loss\n","      losses.update(ce.item(), X.size(0))\n","      errors.update(err, X.size(0))\n","\n","      # measure elapsed time\n","      batch_time.update(time.time() - end)\n","      end = time.time()\n","\n","      if verbose and i % verbose == 0:\n","          print('Test: [{0}/{1}]\\t'\n","                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                'Error {error.val:.3f} ({error.avg:.3f})'.format(\n","                    i, len(test_loader), batch_time=batch_time, loss=losses,\n","                    error=errors))\n","      test_log.flush()\n","\n","  print(' * Error {error.avg:.4f}\\t'\n","        'Loss {loss.avg:.4f}'\n","        .format(error=errors, loss=losses))\n","  torch.save(model.state_dict(), prefix +str(epoch)+ \"_Baseline_model.pth\")\n","\n","#torch.save(model.state_dict(), prefix + \"F_Baseline_model.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7rsnF0tsaTX","colab_type":"text"},"source":["# Test Baseline"]},{"cell_type":"code","metadata":{"id":"iESLAn7XcXda","colab_type":"code","colab":{}},"source":["model = CNN_model().cpu()\n","model.load_state_dict(torch.load('/content/SVHN_robust/0_Baseline_model.pth'))\n","model.eval()\n","\n","x_acc_test_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy')\n","x_acc_test=[]\n","for i in range(26032):\n","  test = x_acc_test_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_acc_test.append(np.array(test_new))\n","x_acc_test = np.array(x_acc_test)\n","\n","y_test_acc=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_acc_test.npy')\n","\n","tensor_x_test_acc = torch.tensor(x_acc_test) # transform to torch tensor\n","tensor_y_test_acc = torch.tensor(y_test_acc, dtype=torch.long)\n","dataset_test_acc = TensorDataset(tensor_x_test_acc, tensor_y_test_acc) # create your datset\n","test_loader_acc = DataLoader(dataset_test_acc, batch_size=20, shuffle=False) # create your dataloader\n","\n","correct = 0\n","total = 0\n","predictions = []\n","with torch.no_grad():\n","    for data in test_loader_acc:\n","        images, labels = data\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 26032 test images: %f %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9Wt5C2p94S1","colab_type":"code","colab":{}},"source":["print (model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TS6giVIysdMY","colab_type":"text"},"source":["# Baseline From PyTorch to Keras"]},{"cell_type":"code","metadata":{"id":"1yVWbSqT93Uk","colab_type":"code","colab":{}},"source":["def get_CNN_model():\n","  #CNN network for classification\n","  svhn_model = models.Sequential(name='CNN')\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', input_shape=(3, 32, 32),  name='0'))\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', name='2'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='4'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='6'))\n","  svhn_model.add(Flatten(name='8'))\n","  svhn_model.add(layers.Dense(100, activation='relu', name='9'))\n","  svhn_model.add(layers.Dense(10, name='11'))\n","  return svhn_model\n","\n","keras_model = get_CNN_model()\n","keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[categorical_accuracy])\n","keras_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAsyACqPkYWO","colab_type":"code","colab":{}},"source":["keras_model = get_CNN_model()\n","keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[categorical_accuracy])\n","\n","model = CNN_model()\n","model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/0_Baseline_model.pth'))\n","model.eval()\n","\n","keras_model = pyt_to_keras(model, keras_model)\n","keras_model.save('/content/SVHN_robust/Baseline_Keras_78.h5')\n","print ('Keras Model Saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Id3bwd8Vkpyo","colab_type":"code","colab":{}},"source":["# Evaluate CNN\n","x_acc_test_cnn = np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy')\n","y_acc_test=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_acc_test.npy')\n","\n","keras_model = get_CNN_model()\n","keras_model = keras.models.load_model('/content/SVHN_robust/Baseline_Keras_78.h5')\n","score, acc = keras_model.evaluate(x_acc_test, y_acc_test , batch_size=128)\n","print (\"Test Accuracy: %.5f\" % acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Ouq6r6uftVW","colab_type":"code","colab":{}},"source":["data = torch.rand(6,3,32,32)\n","data_keras = data.numpy()\n","data_pytorch = Variable(data, requires_grad=False)\n","\n","# Do a forward pass in both frameworks\n","keras_pred = keras_model.predict(data_keras)\n","pytorch_pred = model(data_pytorch).data.numpy()\n","assert keras_pred.shape == pytorch_pred.shape\n","\n","plt.axis('Off')\n","plt.imshow(keras_pred)\n","plt.show()\n","plt.axis('Off')\n","plt.imshow(pytorch_pred)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T54DFyOWuXfi","colab_type":"text"},"source":["# Train Robustly"]},{"cell_type":"code","metadata":{"id":"hIP0x_ILKt7c","colab_type":"code","colab":{}},"source":["def CNN_model():\n","  model = nn.Sequential(\n","        nn.Conv2d(3, 16,  4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 32, 4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Linear(32*8*8,100),\n","        nn.ReLU(),\n","        nn.Linear(100, 10)\n","    )\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rgNcfsmsjKk","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import setproctitle\n","import argparse\n","import problems as pblm\n","from trainer import *\n","import torch\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import shutil\n","\n","prefix = \"/content/SVHN_robust/\"\n","\n","train_log = open(prefix + \"train_robust.log\", \"w\")\n","test_log = open(prefix + \"test_robust.log\", \"w\")\n","\n","# Restore the data\n","x_train_cnn_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_train_cnn.npy')\n","x_train_cnn=[]\n","for i in range(58605):\n","  test = x_train_cnn_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_train_cnn.append(np.array(test_new))\n","x_train_cnn = np.array(x_train_cnn) \n","\n","x_test_cnn_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_test_cnn.npy')\n","x_test_cnn=[]\n","for i in range(14652):\n","  test = x_test_cnn_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_test_cnn.append(np.array(test_new))\n","x_test_cnn = np.array(x_test_cnn)\n","\n","y_train=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_train.npy')\n","y_test=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_test.npy')\n","\n","tensor_x = torch.tensor(x_train_cnn, dtype=torch.float) # transform to torch tensor\n","tensor_y = torch.tensor(y_train, dtype=torch.long)\n","dataset_train = TensorDataset(tensor_x, tensor_y) # create your datset\n","train_loader = DataLoader(dataset_train,batch_size=20, shuffle=False) # create your dataloader\n","\n","tensor_x_test = torch.tensor(x_test_cnn) # transform to torch tensor\n","tensor_y_test = torch.tensor(y_test, dtype=torch.long)\n","dataset_test = TensorDataset(tensor_x_test, tensor_y_test) # create your datset\n","test_loader = DataLoader(dataset_test, batch_size=20, shuffle=False) # create your dataloader\n","\n","train_loader, test_loader = pblm.svhn_loaders(20)\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","model = CNN_model().cuda()\n","#model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/1_Robust_model.pth'))\n","\n","opt = optim.Adam(model.parameters(), lr=0.001)\n","\n","epsilon_final = 0.01\n","batch_size=20\n","epochs=15\n","\n","alpha_grad=False\n","scatter_grad=False\n","\n","verbose = 0\n","real_time=False\n","parallel=True\n","\n","starting_epsilon = 0.001\n","\n","for epoch in range (epochs):\n","  batch_time = AverageMeter()\n","  data_time = AverageMeter()\n","  losses = AverageMeter()\n","  errors = AverageMeter()\n","  robust_losses = AverageMeter()\n","  robust_errors = AverageMeter()\n","\n","  model.train()\n","\n","  end = time.time()\n","  for i, (X,y) in enumerate(train_loader):\n","      X,y = X.cuda(), y.cuda().long()\n","      if y.dim() == 2:\n","          y = y.squeeze(1)\n","      data_time.update(time.time() - end)\n","\n","      with torch.no_grad():\n","          out = model(Variable(X))\n","          ce = nn.CrossEntropyLoss()(out, Variable(y))\n","          err = (out.max(1)[1] != y).float().sum()  / X.size(0)\n","\n","      if epoch <= epochs//2 and starting_epsilon is not None: \n","        epsilon = starting_epsilon + (epoch/(epochs//2))*(epsilon_final - starting_epsilon)\n","      else:\n","        epsilon = epsilon_final\n","\n","      robust_ce, robust_err = robust_loss(model, epsilon, Variable(X), Variable(y))\n","      opt.zero_grad()\n","\n","      robust_ce.backward(torch.ones_like(robust_ce))\n","\n","      opt.step()\n","\n","      # measure accuracy and record loss\n","      losses.update(ce.item(), X.size(0))\n","      errors.update(err.item(), X.size(0))\n","      robust_losses.update(robust_ce.detach().item(), X.size(0))\n","      robust_errors.update(robust_err, X.size(0))\n","\n","      # measure elapsed time\n","      batch_time.update(time.time()-end)\n","      end = time.time()\n","\n","      print(epoch, i, robust_ce.detach().item(),\n","              robust_err, ce.item(), err.item(), file=train_log)\n","\n","      if verbose and (i % verbose == 0 or real_time):\n","          endline = '\\n' if i % verbose == 0 else '\\r'\n","          print('Epoch: [{0}][{1}/{2}]\\t'\n","                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                'Robust loss {rloss.val:.4f} ({rloss.avg:.4f})\\t'\n","                'Robust error {rerrors.val:.3f} ({rerrors.avg:.3f})\\t'\n","                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                'Error {errors.val:.3f} ({errors.avg:.3f})'\n","                .format(\n","                  epoch, i, len(train_loader), \n","                  batch_time=batch_time,\n","                  data_time=data_time,\n","                  loss=losses,\n","                  errors=errors,\n","                  rloss = robust_losses,\n","                  rerrors = robust_errors)\n","                , end=endline)\n","      train_log.flush()\n","\n","      del X, y, robust_ce, out, ce, err, robust_err\n","        \n","  torch.cuda.empty_cache()\n","\n","  batch_time = AverageMeter()\n","  losses = AverageMeter()\n","  errors = AverageMeter()\n","  robust_losses = AverageMeter()\n","  robust_errors = AverageMeter()\n","\n","  model.eval()\n","\n","  end = time.time()\n","\n","  torch.set_grad_enabled(False)\n","  for i, (X,y) in enumerate(test_loader):\n","      X,y = X.cuda(), y.cuda().long()\n","      if y.dim() == 2: \n","          y = y.squeeze(1)\n","\n","      robust_ce, robust_err = robust_loss(model, epsilon_final, X, y)\n","\n","      out = model(Variable(X))\n","      ce = nn.CrossEntropyLoss()(out, Variable(y))\n","      err = (out.max(1)[1] != y).float().sum()  / X.size(0)\n","\n","      # measure accuracy and record loss\n","      losses.update(ce.item(), X.size(0))\n","      errors.update(err, X.size(0))\n","      robust_losses.update(robust_ce.item(), X.size(0))\n","      robust_errors.update(robust_err, X.size(0))\n","\n","      # measure elapsed time\n","      batch_time.update(time.time()-end)\n","      end = time.time()\n","\n","      print(epoch, i, robust_ce.item(), robust_err, ce.item(), err.item(),\n","          file=test_log)\n","      if verbose: \n","          endline = '\\n' if i % verbose == 0 else '\\r'\n","          print('Test: [{0}/{1}]\\t'\n","                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                'Robust loss {rloss.val:.3f} ({rloss.avg:.3f})\\t'\n","                'Robust error {rerrors.val:.3f} ({rerrors.avg:.3f})\\t'\n","                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                'Error {error.val:.3f} ({error.avg:.3f})'.format(\n","                    i, len(test_loader), batch_time=batch_time, \n","                    loss=losses, error=errors, rloss = robust_losses, \n","                    rerrors = robust_errors), end=endline)\n","      test_log.flush()\n","\n","      del X, y, robust_ce, out, ce\n","\n","  torch.set_grad_enabled(True)\n","  torch.cuda.empty_cache()\n","  print(str(epoch))\n","  print(' * Robust error {rerror.avg:.3f}\\t'\n","        'Error {error.avg:.3f}\\t'\n","        'Robust loss {rloss.avg:.3f}\\t'\n","        'Loss {loss.avg:.4f}'\n","        .format(rerror=robust_errors, error=errors, rloss=robust_losses, loss=losses))\n","  print (' ')\n","\n","  torch.save(model.state_dict(), prefix +str(epoch)+ \"_!Robust_model.pth\")\n","  shutil.move(prefix +str(epoch)+ \"_!Robust_model.pth\", \"/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/\"+str(epoch)+ \"_!Robust_model.pth\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dElKGcQCqdX","colab_type":"text"},"source":["# Test Robustly"]},{"cell_type":"code","metadata":{"id":"VT3bRezSkqL-","colab_type":"code","colab":{}},"source":["model = CNN_model()\n","model.load_state_dict(torch.load('/content/3_Robust_model.pth'))\n","model.eval()\n","\n","x_acc_test_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy')\n","x_acc_test=[]\n","for i in range(26032):\n","  test = x_acc_test_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_acc_test.append(np.array(test_new))\n","x_acc_test = np.array(x_acc_test)\n","\n","y_test_acc=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_acc_test.npy')\n","\n","tensor_x_test_acc = torch.tensor(x_acc_test) # transform to torch tensor\n","tensor_y_test_acc = torch.tensor(y_test_acc, dtype=torch.long)\n","dataset_test_acc = TensorDataset(tensor_x_test_acc, tensor_y_test_acc) # create your datset\n","test_loader_acc = DataLoader(dataset_test_acc, batch_size=20, shuffle=False) # create your dataloader\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader_acc:\n","        images, labels = data\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the test images: %4f %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_UVWvvbkDHRJ","colab_type":"text"},"source":["# Robust From PyTorch to Keras"]},{"cell_type":"code","metadata":{"id":"g19RxSRalwbM","colab_type":"code","colab":{}},"source":["def get_CNN_model():\n","  #CNN network for classification\n","  svhn_model = models.Sequential(name='CNN')\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', input_shape=(3, 32, 32),  name='0'))\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', name='2'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='4'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='6'))\n","  svhn_model.add(Flatten(name='8'))\n","  svhn_model.add(layers.Dense(100, activation='relu', name='9'))\n","  svhn_model.add(layers.Dense(10, name='11'))\n","  return svhn_model\n","\n","keras_model = get_CNN_model()\n","keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[categorical_accuracy])\n","keras_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWR1-x26ja6c","colab_type":"code","colab":{}},"source":["def get_SCNN_model_layer2(channels_l1, channels_l2):\n","  #CNN network for classification\n","  svhn_model = models.Sequential(name='SCNN')\n","  svhn_model.add(layers.Conv2D(channels_l1, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', input_shape=(3, 32, 32),  name='0'))\n","  svhn_model.add(layers.Conv2D(channels_l2, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', name='2'))\n","  svhn_model.add(Flatten(name='4'))\n","  svhn_model.add(layers.Dense(100, activation='relu', name='5'))\n","  svhn_model.add(layers.Dense(10, name=\"7\"))\n","  return svhn_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpnds62Zlwmg","colab_type":"code","colab":{}},"source":["keras_model = get_SCNN_model_layer2(16, 32)\n","keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[categorical_accuracy])\n","\n","model = CNN_model()\n","model.load_state_dict(torch.load('/content/3_Robust_model.pth'))\n","model.eval()\n","\n","keras_model = pyt_to_keras(model, keras_model)\n","keras_model.save('/content/Robust_ReTrain_Keras.h5')\n","print ('Keras Model Saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCL-gwfblwqE","colab_type":"code","colab":{}},"source":["# Evaluate CNN\n","x_acc_test_cnn = np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy')\n","y_acc_test=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_acc_test.npy')\n","\n","keras_model = get_SCNN_model_layer2(16, 32)\n","keras_model = keras.models.load_model('/content/Robust_ReTrain_Keras.h5')\n","score, acc = keras_model.evaluate(x_acc_test, y_acc_test , batch_size=128)\n","print (\"Test Accuracy: %.5f\" % acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfuXUcjmlwzh","colab_type":"code","colab":{}},"source":["data = torch.rand(6,3,32,32)\n","data_keras = data.numpy()\n","data_pytorch = Variable(data, requires_grad=False)\n","\n","# Do a forward pass in both frameworks\n","keras_pred = keras_model.predict(data_keras)\n","pytorch_pred = model(data_pytorch).data.numpy()\n","assert keras_pred.shape == pytorch_pred.shape\n","\n","plt.axis('Off')\n","plt.imshow(keras_pred)\n","plt.show()\n","plt.axis('Off')\n","plt.imshow(pytorch_pred)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UaAXn120NAG","colab_type":"text"},"source":["# Robust Distillated From Keras to PyTorch"]},{"cell_type":"code","metadata":{"id":"1TOAKWaOgV1p","colab_type":"code","colab":{}},"source":["def get_SCNN_model_layer2(channels_l1, channels_l2):\n","  #CNN network for classification\n","  svhn_model = models.Sequential(name='SCNN')\n","  svhn_model.add(layers.Conv2D(channels_l1, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', input_shape=(3, 32, 32),  name='0'))\n","  svhn_model.add(layers.Conv2D(channels_l2, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', name='2'))\n","  svhn_model.add(Flatten(name='8'))\n","  svhn_model.add(layers.Dense(100, activation='relu', name='9'))\n","  svhn_model.add(layers.Dense(10, name=\"logit\"))\n","  return svhn_model\n","\n","def get_CNN_model():\n","  #CNN network for classification\n","  svhn_model = models.Sequential(name='CNN')\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', input_shape=(3, 32, 32),  name='0'))\n","  svhn_model.add(layers.Conv2D(16, 4, data_format='channels_first', activation='relu', strides = 2, padding='same', name='2'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='4'))\n","  svhn_model.add(layers.Conv2D(32, 4,  data_format='channels_first',activation='relu', strides = 2, padding='same', name='6'))\n","  svhn_model.add(Flatten(name='8'))\n","  svhn_model.add(layers.Dense(100, activation='relu', name='9'))\n","  svhn_model.add(layers.Dense(10, name='11'))\n","  return svhn_model\n","\n","keras_network = get_SCNN_model_layer2(16, 32)\n","keras_network = keras.models.load_model('/content/SCNN_MIMIC_SVHN_h16_32.h5')\n","\n","for i, layer in enumerate(keras_network.layers):\n","    if i == 1:\n","      layer.name = str(0)\n","    elif i == 3:\n","      layer.name = str(4)\n","    elif i == 4:\n","      layer.name = str(5)\n","    elif i == 5:\n","      layer.name = str(7)\n","\n","keras_network.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjk9gz5_g5u8","colab_type":"code","colab":{}},"source":["def CNN_model():\n","  model = nn.Sequential(\n","        nn.Conv2d(3, 16,  4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 32, 4, stride=2, padding=1),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Linear(32*8*8,100),\n","        nn.ReLU(),\n","        nn.Linear(100, 10)\n","    )\n","  return model\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n","model = CNN_model()\n","model.cuda()\n","from torchsummary import summary\n","\n","summary(model, (3, 32, 32))\n","print (model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzUHjLqEhxA5","colab_type":"code","colab":{}},"source":["from nn_transfer import transfer, util\n","\n","pytorch_network = CNN_model()\n","transfer.keras_to_pytorch(keras_network, pytorch_network)\n","torch.save(pytorch_network.state_dict(),\"SCNN_MIMIC_SVHN_h16_32.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-nO8aG5kaOX","colab_type":"code","colab":{}},"source":["x_acc_test_old=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/x_acc_test_cnn.npy')\n","x_acc_test=[]\n","for i in range(26032):\n","  test = x_acc_test_old[i,:,:,:].T\n","  test_new = []\n","  for j in range(3):\n","    test_new.append(test[j].T)\n","  x_acc_test.append(np.array(test_new))\n","x_acc_test = np.array(x_acc_test)\n","\n","y_test_acc=np.load('/content/drive/My Drive/Colab Notebooks/SVHN/robust_convex/y_labels_acc_test.npy')\n","\n","tensor_x_test_acc = torch.tensor(x_acc_test) # transform to torch tensor\n","tensor_y_test_acc = torch.tensor(y_test_acc, dtype=torch.long)\n","dataset_test_acc = TensorDataset(tensor_x_test_acc, tensor_y_test_acc) # create your datset\n","test_loader_acc = DataLoader(dataset_test_acc, batch_size=20, shuffle=False) # create your dataloader\n","\n","model = CNN_model()\n","model.load_state_dict(torch.load('/content/SCNN_MIMIC_SVHN_h16_32.pth'))\n","model.eval()\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader_acc:\n","        images, labels = data\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the test images: %4f %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[]}]}